# LightRAG with Chutes AI Configuration

C·∫•u h√¨nh LightRAG s·ª≠ d·ª•ng Chutes AI l√†m LLM v√† Embedding provider.

## üìÅ C·∫•u tr√∫c th∆∞ m·ª•c

```
~/LightRAG/
‚îú‚îÄ‚îÄ .env                    # Environment configuration
‚îú‚îÄ‚îÄ docker-compose.yml      # Docker Compose setup
‚îú‚îÄ‚îÄ inputs/                 # Directory for input documents
‚îú‚îÄ‚îÄ rag_storage/            # Storage for RAG data
‚îî‚îÄ‚îÄ README-CHUTES.md        # This file
```

## üîß C·∫•u h√¨nh

### 1. L·∫•y Chutes API Key

1. Truy c·∫≠p: https://chutes.ai/
2. ƒêƒÉng nh·∫≠p / T·∫°o t√†i kho·∫£n
3. V√†o **Settings** ‚Üí **API Keys**
4. T·∫°o key m·ªõi v√† copy

### 2. Set API Key

**C√°ch 1: Export environment variable (khuy·∫øn ngh·ªã)**
```bash
export CHUTES_API_KEY=cp-your-api-key-here
```

**C√°ch 2: Th√™m v√†o ~/.bashrc ho·∫∑c ~/.zshrc**
```bash
echo 'export CHUTES_API_KEY=cp-your-api-key-here' >> ~/.bashrc
source ~/.bashrc
```

## üöÄ Ch·∫°y LightRAG

### C√°ch 1: S·ª≠ d·ª•ng Docker (Khuy·∫øn ngh·ªã)

```bash
# Ch·∫°y v·ªõi Docker Compose
docker-compose up -d

# Ki·ªÉm tra logs
docker-compose logs -f

# D·ª´ng service
docker-compose down
```

### C√°ch 2: Ch·∫°y tr·ª±c ti·∫øp v·ªõi Python

```bash
# C√†i ƒë·∫∑t dependencies
pip install -e .

# Ch·∫°y server
lightrag-server
```

## üìù S·ª≠ d·ª•ng

### 1. Upload Documents

```bash
# Copy files v√†o inputs directory
cp /path/to/your/documents/* ./inputs/

# Ho·∫∑c d√πng API
curl -X POST "http://localhost:9621/documents/batch" \
  -H "Content-Type: multipart/form-data" \
  -F "files=@document.pdf"
```

### 2. Query

```bash
# Local mode - specific entities
curl -X POST "http://localhost:9621/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "How does authentication work?",
    "mode": "local"
  }'

# Global mode - big picture
curl -X POST "http://localhost:9621/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is the system architecture?",
    "mode": "global"
  }'

# Hybrid mode - best of both
curl -X POST "http://localhost:9621/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Find all usages of UserService",
    "mode": "hybrid"
  }'
```

### 3. WebUI

Truy c·∫≠p: http://localhost:9621/webui

## ‚öôÔ∏è Configuration Details

### LLM Configuration (Chutes)
- **Provider**: Chutes AI
- **Model**: `chutesai/llama-4-scout` (128K context)
- **API**: https://api.chutes.ai/v1
- **Max Tokens**: 9000

### Embedding Configuration (Chutes)
- **Provider**: Chutes AI
- **Model**: `chutesai/nomic-embed-text`
- **Dimensions**: 768
- **API**: https://api.chutes.ai/v1

### Available Chutes Models

| Model | Context | Use Case |
|-------|---------|----------|
| `chutesai/llama-4-scout` | 128K | General purpose, fast |
| `chutesai/llama-4-maverick` | 128K | Complex reasoning |
| `chutesai/deepseek-v3` | 64K | Code, analysis |
| `chutesai/qwen-2.5-72b` | 32K | Multilingual |
| `chutesai/nomic-embed-text` | - | Embeddings |

ƒê·ªÉ ƒë·ªïi model, edit `.env` v√† restart:
```bash
LLM_MODEL=chutesai/llama-4-maverick
```

## üîç Query Modes

| Mode | Description | Best For |
|------|-------------|----------|
| `naive` | Vector similarity only | Simple Q&A |
| `local` | Entity-centric retrieval | Specific entities |
| `global` | Relationship-focused | Big picture questions |
| `hybrid` | Combined approach | Complex queries |
| `mix` | Merge KG + vector results | Balanced results |
| `bypass` | Direct LLM | Quick answers |

## üõ†Ô∏è Troubleshooting

### Issue: "Invalid API Key"
```bash
# Ki·ªÉm tra API key ƒë√£ set ch∆∞a
echo $CHUTES_API_KEY

# N·∫øu ch∆∞a, set l·∫°i
export CHUTES_API_KEY=cp-your-api-key-here
```

### Issue: "Connection refused"
```bash
# Ki·ªÉm tra container ƒëang ch·∫°y
docker ps

# Restart n·∫øu c·∫ßn
docker-compose restart
```

### Issue: "Rate limit"
- Chutes c√≥ rate limits cho free tier
- Gi·∫£m `MAX_ASYNC` v√† `MAX_PARALLEL_INSERT` trong `.env`
- Ho·∫∑c n√¢ng c·∫•p plan

## üìä Monitoring

```bash
# Health check
curl http://localhost:9621/health

# Pipeline status
curl http://localhost:9621/documents/pipeline_status

# Graph stats
curl "http://localhost:9621/graph?label=list"
```

## üîó Integration v·ªõi OpenCode

Th√™m v√†o OpenCode MCP configuration:

```json
{
  "mcpServers": {
    "lightrag": {
      "url": "http://localhost:9621",
      "headers": {
        "X-API-Key": "your-lightrag-api-key"
      }
    }
  }
}
```

## üìù Notes

- D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u trong `./rag_storage/` (persisted qua Docker volumes)
- LLM cache ƒë∆∞·ª£c enable ƒë·ªÉ ti·∫øt ki·ªám tokens
- S·ª≠ d·ª•ng JSON storage (file-based) cho testing
- ƒê·ªïi sang PostgreSQL cho production

## üìö Resources

- [LightRAG GitHub](https://github.com/HKUDS/LightRAG)
- [Chutes AI](https://chutes.ai/)
- [Chutes API Docs](https://docs.chutes.ai/)
